{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T11:16:16.280992Z",
     "start_time": "2024-04-08T11:16:14.025743Z"
    }
   },
   "source": [
    "#numpy sklearn jieba gensim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer #载入词袋模型特征\n",
    "from sklearn.feature_extraction.text import TfidfTransformer #载入Tfidf转换器\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #载入Tfidf特征\n",
    "import gensim\n",
    "from sklearn.naive_bayes import MultinomialNB #导入多项朴素贝叶斯分类算法\n",
    "from sklearn.linear_model import SGDClassifier #导入随机梯度下降分类器\n",
    "from sklearn.linear_model import LogisticRegression #导入逻辑回归分类器\n",
    "from sklearn import metrics"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T11:16:16.301414Z",
     "start_time": "2024-04-08T11:16:16.282070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    获取数据\n",
    "    :return：文本数据，对应的labels\n",
    "    \"\"\"\n",
    "    with open('./dataset/data/ham_data.txt', encoding='utf-8') as ham_f, open('./dataset/data/spam_data.txt',\n",
    "                                                                              encoding='utf-8') as spam_f:\n",
    "        ham_data = ham_f.readlines()\n",
    "        spam_data = spam_f.readlines()\n",
    "\n",
    "        ham_label = np.ones(len(ham_data)).tolist()\n",
    "        spam_label = np.zeros(len(spam_data)).tolist()\n",
    "\n",
    "        corpus = ham_data + spam_data\n",
    "\n",
    "        labels = ham_label + spam_label\n",
    "\n",
    "    return corpus, labels\n",
    "\n",
    "\n",
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    \"\"\"\n",
    "    :param corpus:\n",
    "    :param labels:\n",
    "    :param test_data_proportion:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_x, test_x, train_y, test_y = train_test_split(corpus, labels, test_size=test_data_proportion, random_state=42)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_corpus, filtered_labels\n",
    "\n",
    "\n",
    "corpus, labels = get_data()\n",
    "\n",
    "print(f'总的数据量：{len(labels)}')\n",
    "\n",
    "corpus, labels = remove_empty_docs(corpus, labels)\n",
    "\n",
    "print(f'样本之一：{corpus[10]}')\n",
    "print(f'样本的label：{labels[10]}')\n",
    "label_name_map = ['垃圾邮件', '正常邮件']\n",
    "print(f'实际类型：{label_name_map[int(labels[10])]},'\n",
    "      f'{label_name_map[int(labels[5900])]}')\n",
    "\n",
    "# 对数据进行划分\n",
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus, labels, test_data_proportion=0.3)\n"
   ],
   "id": "17e6d9ae249ce22e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的数据量：10001\n",
      "样本之一：北京售票员可厉害，嘿嘿，有专座的，会直接拉着脖子指着鼻子让上面的人站起来让 座的，呵呵，比较赞。。。 杭州就是很少有人给让座，除非司机要求乘客那样做。 五一去杭州一个景点玩，车上有两个不到一岁的小孩，就是没有人给让座，没办法家长只能在车上把小孩的推车打开让孩子坐进去，但是孩子还是闹，只能抱着，景点离市区很远，车上很颠，最后家长坐在地上抱孩子，就是没有一个人给让座，要是在北京，一上车就有人让座了\n",
      "\n",
      "样本的label：1.0\n",
      "实际类型：正常邮件,垃圾邮件\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T11:16:16.307709Z",
     "start_time": "2024-04-08T11:16:16.302863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('./dataset/stop_words.utf8', encoding='utf-8') as f:\n",
    "    stopword_list = f.readlines()\n",
    "\n",
    "\n",
    "# jieba分词\n",
    "def tokenize_text(text):\n",
    "    tokens = jieba.lcut(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# 去除特殊字符\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "# 去除停用词\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "# 当tokenize=True时候，会对文本进行分词，除此以外还会对文本进行去除特殊符号、停用词等预处理\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "    return normalized_corpus\n"
   ],
   "id": "71156a7cef2cddfc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T11:16:38.124300Z",
     "start_time": "2024-04-08T11:16:16.309989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bow_extractor(corpus, ngram_range=(1, 1)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "\n",
    "# Tfidf特征转换\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    transformer = TfidfTransformer(norm='l2', smooth_idf=True, use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "\n",
    "\n",
    "# tfidf特征提取\n",
    "def tfidf_extractor(corpus, ngram_range=(1, 1)):\n",
    "    vectorizer = TfidfVectorizer(min_df=1, norm='l2', smooth_idf=True, use_idf=True, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "\n",
    "# 进行归一化\n",
    "norm_train_corpus = normalize_corpus(train_corpus)\n",
    "norm_test_corpus = normalize_corpus(test_corpus)\n",
    "\n",
    "# 词袋模型特征\n",
    "bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "# tf-idf特征\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "# 训练数据tokenize化\n",
    "tokenized_train = [jieba.lcut(text) for text in norm_train_corpus]\n",
    "print(tokenized_train[2:10])\n",
    "tokenized_test = [jieba.lcut(text) for text in norm_test_corpus]\n",
    "\n",
    "# build word2vec模型\n",
    "model = gensim.models.Word2Vec(tokenized_train, window=100, min_count=30, sample=1e-3)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge')\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "# 首先声明了mnb, svm, lr 三个分类器，然后传入train_predict_evaluate_model函数中\n",
    "def train_predict_evaluate_model(classifier, train_features, train_labels, test_features, test_labels):\n",
    "    # build model\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features)\n",
    "    # evaluate model prediction performance\n",
    "    get_metrics(true_labels=test_labels, predicted_labels=predictions)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('Accuracy:', np.round(metrics.accuracy_score(true_labels, predicted_labels), 2))\n",
    "    print('Precision:', np.round(metrics.precision_score(true_labels, predicted_labels, average='weighted'), 2))\n",
    "    print('F1 Score:', np.round(metrics.f1_score(true_labels, predicted_labels, average='weighted'), 2))\n"
   ],
   "id": "248c1bb5174e3b22",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/l9/llzdgt2s3hz2rfc6cjp7wgb40000gn/T/jieba.cache\n",
      "Loading model cost 0.351 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['10156', ' ', ' ', '说', ' ', ' ', '的', ' ', ' ', '呵呵', ' ', ' ', '标', ' ', ' ', '题', ' ', ' ', 'Re', ' ', ' ', '我', ' ', ' ', '要是', ' ', ' ', '你', ' ', ' ', '女朋友', ' ', ' ', '绝对', ' ', ' ', '跟', ' ', ' ', '你', ' ', ' ', '分手', ' ', ' ', 'Re', ' ', ' ', '昨晚', ' ', ' ', '猫', ' ', ' ', '又', ' ', ' ', '闹', ' ', ' ', '了', ' ', ' ', '一夜', ' ', ' ', '嗯', ' ', ' ', '，', ' ', ' ', '谁', ' ', ' ', '说', ' ', ' ', '我', ' ', ' ', '养猫', ' ', ' ', '是因为', ' ', ' ', '有', ' ', ' ', '爱心', ' ', ' ', '我', ' ', ' ', '跟', ' ', ' ', '谁', ' ', ' ', '急', ' ', ' ', '是', ' ', ' ', '哈', ' ', ' ', '，', ' ', ' ', '不是', ' ', ' ', '用', ' ', ' ', '爱心', ' ', ' ', '区分', ' ', ' ', '的', ' ', ' ', '喜欢', ' ', ' ', '宠物', ' ', ' ', '的', ' ', ' ', '就', ' ', ' ', '养', ' ', ' ', '，', ' ', ' ', '不', ' ', ' ', '喜欢', ' ', ' ', '的', ' ', ' ', '就', ' ', ' ', '不养', ' ', ' ', '呗', ' ', ' ', '卷卷', ' ', ' ', '，', ' ', ' ', '你', ' ', ' ', '搞', ' ', ' ', '成', ' ', ' ', '这', ' ', ' ', '副', ' ', ' ', '鬼', ' ', ' ', '样子', ' ', ' ', '，', ' ', ' ', '还', ' ', ' ', '好意思', ' ', ' ', '来', ' ', ' ', '找', ' ', ' ', '我', ' ', ' ', '撒娇', ' ', ' ', '。', ' ', ' ', '。', ' ', ' ', '。'], ['中信', ' ', ' ', '（', ' ', ' ', '国际', ' ', ' ', '）', ' ', ' ', '电子科技', ' ', ' ', '有限公司', ' ', ' ', '推出', ' ', ' ', '新', ' ', ' ', '产品', ' ', ' ', '：', ' ', ' ', '升职', ' ', ' ', '步步高', ' ', ' ', '、', ' ', ' ', '做生意', ' ', ' ', '发大财', ' ', ' ', '、', ' ', ' ', '连', ' ', ' ', '找', ' ', ' ', '情人', ' ', ' ', '都', ' ', ' ', '用', ' ', ' ', '的', ' ', ' ', '上', ' ', ' ', '，', ' ', ' ', '详情', ' ', ' ', '进入', ' ', ' ', '网', ' ', ' ', '址', ' ', ' ', 'http', ' ', ' ', 'www', ' ', ' ', 'usa5588', ' ', ' ', 'com', ' ', ' ', 'ccc', ' ', ' ', '电话', ' ', ' ', '：', ' ', ' ', '020', ' ', ' ', '33770208', ' ', ' ', '服务', ' ', ' ', '热线', ' ', ' ', '：', ' ', ' ', '013650852999'], ['贵', ' ', ' ', '公司', ' ', ' ', '负责人', ' ', ' ', '：', ' ', ' ', '你好', ' ', ' ', '！', ' ', ' ', '本', ' ', ' ', '公司', ' ', ' ', '祥泰', ' ', ' ', '实业', ' ', ' ', '有限公司', ' ', ' ', '）', ' ', ' ', '具有', ' ', ' ', '进出口', ' ', ' ', '及', ' ', ' ', '国内贸易', ' ', ' ', '的', ' ', ' ', '企业', ' ', ' ', '承', ' ', ' ', '多家', ' ', ' ', '公司', ' ', ' ', '委托', ' ', ' ', '有', ' ', ' ', '广告', ' ', ' ', '建筑工程', ' ', ' ', '其它', ' ', ' ', '服务', ' ', ' ', '商品销售', ' ', ' ', '等', ' ', ' ', '的', ' ', ' ', '发票', ' ', ' ', '向', ' ', ' ', '外代', ' ', ' ', '开', ' ', ' ', '点数', ' ', ' ', '优惠', ' ', ' ', '本', ' ', ' ', '公司', ' ', ' ', '原则', ' ', ' ', '是', ' ', ' ', '满意', ' ', ' ', '后', ' ', ' ', '付款', ' ', ' ', '有意者', ' ', ' ', '请来', ' ', ' ', '电', ' ', ' ', '洽谈', ' ', ' ', '电', ' ', ' ', '话', ' ', ' ', '：', ' ', ' ', '013631690076', ' ', ' ', '邮', ' ', ' ', '箱', ' ', ' ', '：', ' ', ' ', 'shitailong8', ' ', ' ', '163', ' ', ' ', 'com', ' ', ' ', '联系人', ' ', ' ', '：', ' ', ' ', '郭', ' ', ' ', '生', ' ', ' ', '如', ' ', ' ', '给', ' ', ' ', '贵', ' ', ' ', '公司', ' ', ' ', '带来', ' ', ' ', '不便', ' ', ' ', '请谅解'], ['李敖来', ' ', ' ', '大陆', ' ', ' ', '，', ' ', ' ', '轰轰烈烈', ' ', ' ', '，', ' ', ' ', '热热闹闹', ' ', ' ', '，', ' ', ' ', '国内', ' ', ' ', '有些', ' ', ' ', '人', ' ', ' ', '也', ' ', ' ', '坐不住', ' ', ' ', '了', ' ', ' ', '，', ' ', ' ', '总', ' ', ' ', '有人', ' ', ' ', '要', ' ', ' ', '跳', ' ', ' ', '出来', ' ', ' ', '显摆', ' ', ' ', '显摆', ' ', ' ', '，', ' ', ' ', '不过', ' ', ' ', '，', ' ', ' ', '没显', ' ', ' ', '好', ' ', ' ', '倒', ' ', ' ', '是', ' ', ' ', '现', ' ', ' ', '了', ' ', ' ', '眼', ' ', ' ', '。', ' ', ' ', '大家', ' ', ' ', '看看', ' ', ' ', '下面', ' ', ' ', '的', ' ', ' ', '材料', ' ', ' ', '，', ' ', ' ', '真', ' ', ' ', '为', ' ', ' ', '清华', ' ', ' ', '汗颜', ' ', ' ', '啊', ' ', ' ', '！', ' ', ' ', '资料', ' ', ' ', '一', ' ', ' ', '【', ' ', ' ', '清华大学', ' ', ' ', '学者', ' ', ' ', '评李敖', ' ', ' ', '演讲', ' ', ' ', '：', ' ', ' ', '李敖', ' ', ' ', '老', ' ', ' ', '矣', ' ', ' ', '尚', ' ', '能', ' ', ' ', '骂否', ' ', ' ', '？', ' ', ' ', '】', ' ', ' ', 'http', ' ', ' ', 'news', ' ', ' ', 'anhuinews', ' ', ' ', 'com', ' ', ' ', 'system', ' ', ' ', '2005', ' ', ' ', '09', ' ', ' ', '23', ' ', ' ', '001359039', ' ', ' ', 'shtml'], ['公司', ' ', ' ', '名称', ' ', ' ', '：', ' ', ' ', '北京', ' ', ' ', '优力', ' ', ' ', '维尔', ' ', ' ', '科技', ' ', ' ', '有限公司', ' ', ' ', '高科技', ' ', ' ', '公司', ' ', ' ', '招聘', ' ', ' ', '行政助理', ' ', ' ', '兼', ' ', ' ', '前台', ' ', ' ', '一名', ' ', ' ', '要求', ' ', ' ', '：', ' ', ' ', '25', ' ', ' ', '岁', ' ', ' ', '以下', ' ', ' ', '，', ' ', ' ', '形象', ' ', ' ', '良好', ' ', ' ', '，', ' ', ' ', '有', ' ', ' ', '责任心', ' ', ' ', '、', ' ', ' ', '诚实', ' ', ' ', '守信', ' ', ' ', '、', ' ', ' ', '有', ' ', ' ', '团队', ' ', ' ', '意识', ' ', ' ', '、', ' ', ' ', '敬业', ' ', ' ', '，', ' ', ' ', '可以', ' ', ' ', '尽快', ' ', ' ', '就位', ' ', ' ', '工作', ' ', ' ', '地点', ' ', ' ', '：', ' ', ' ', '中信', ' ', ' ', '国安', ' ', ' ', '数码港', ' ', ' ', '，', ' ', ' ', '地理位置', ' ', ' ', '在', ' ', ' ', '稻香', ' ', ' ', '园桥', ' ', ' ', '桥东', ' ', ' ', '，', ' ', ' ', '中国航天', ' ', ' ', '大厦', ' ', ' ', '西边', ' ', ' ', '待遇', ' ', ' ', '面议', ' ', ' ', '联系方式', ' ', ' ', '：', ' ', ' ', '简历', ' ', ' ', 'mail', ' ', ' ', 'to', ' ', ' ', 'wuwei', ' ', ' ', '99', ' ', ' ', 'tsinghua', ' ', ' ', 'org', ' ', ' ', 'cn', ' ', ' ', '公司', ' ', ' ', '的', ' ', ' ', '新', ' ', ' ', '网站', ' ', ' ', '和', ' ', ' ', 'mail', ' ', ' ', '服务器', ' ', ' ', '都', ' ', ' ', '在', ' ', ' ', '建设', ' ', ' ', '中', ' ', ' ', '，', ' ', ' ', '投递', ' ', ' ', '的', ' ', ' ', '简历', ' ', ' ', '中', ' ', ' ', '最好', ' ', ' ', '附带', ' ', ' ', '照片', ' ', ' ', '联系电话', ' ', ' ', '：', ' ', ' ', '82652066', ' ', ' ', '工作日', ' ', ' ', '9', ' ', ' ', '00', ' ', ' ', '18', ' ', ' ', '00', ' ', ' ', '欢迎', ' ', ' ', '投递', ' ', ' ', '简历', ' ', ' ', '，', ' ', ' ', '合则', ' ', ' ', '约见'], ['如果', ' ', ' ', '他', ' ', ' ', '只是', ' ', ' ', '想', ' ', ' ', '找个', ' ', ' ', '上床', ' ', ' ', '的', ' ', ' ', '，', ' ', ' ', '这招', ' ', ' ', '可能', ' ', ' ', '管用', ' ', ' ', '，', ' ', ' ', '但', ' ', ' ', '对', ' ', ' ', '他', ' ', ' ', '身边', ' ', ' ', '的', ' ', ' ', '女人', ' ', ' ', '，', ' ', ' ', '这招', ' ', ' ', '不', ' ', ' ', '适用', ' ', ' ', '，', ' ', ' ', '聪明', ' ', ' ', '的', ' ', ' ', '老板', ' ', ' ', '不吃', ' ', ' ', '窝边草', ' ', ' ', '。', ' ', ' ', '是否', ' ', ' ', '只', ' ', ' ', '上床', ' ', ' ', '从来不', ' ', ' ', '由', ' ', ' ', '男人', ' ', ' ', '单方面', ' ', ' ', '说了算', ' ', ' ', '。', ' ', ' ', '如果', ' ', ' ', '是', ' ', ' ', '的话', ' ', ' ', '，', ' ', ' ', '呵呵', ' ', ' ', '，', ' ', ' ', '恐怕', ' ', ' ', '结婚', ' ', ' ', '率会', ' ', ' ', '大大降低', ' ', ' ', '的', ' ', ' ', '。', ' ', ' ', '窝边草', ' ', ' ', '心理', ' ', ' ', '是', ' ', ' ', '个', ' ', ' ', '阻碍', ' ', ' ', '，', ' ', ' ', '克服', ' ', ' ', '它', ' ', ' ', '就是', ' ', ' ', '了', ' ', ' ', '。', ' ', ' ', '个人', ' ', ' ', '猜测', ' ', ' ', '，', ' ', ' ', '这种', ' ', ' ', '人', ' ', ' ', '一般', ' ', ' ', '喜欢', ' ', ' ', '能', ' ', ' ', '给', ' ', ' ', '他', ' ', ' ', '安全感', ' ', ' ', '的', ' ', ' ', '女人', ' ', ' ', '，', ' ', ' ', '即', ' ', ' ', '传统', ' ', ' ', '的', ' ', ' ', '贤妻良母', ' ', ' ', '。', ' ', ' ', '至少', ' ', ' ', '，', ' ', ' ', '他们', ' ', ' ', '更', ' ', ' ', '倾向', ' ', ' ', '于', ' ', ' ', '找', ' ', ' ', '此类', ' ', ' ', '女人', ' ', ' ', '做', ' ', ' ', '老婆', ' ', ' ', '。', ' ', ' ', '即使', ' ', ' ', '他们', ' ', ' ', '骨子里', ' ', ' ', '喜欢', ' ', ' ', '风骚', ' ', ' ', '的', ' ', ' ', '女人', ' ', ' ', '，', ' ', ' ', '呵呵', ' ', ' ', '。', ' ', ' ', '这个', ' ', ' ', '猜测', ' ', ' ', '才', ' ', ' ', '没道理', ' ', ' ', '。', ' ', ' ', '你', ' ', ' ', '在', ' ', ' ', '毫无道理', ' ', ' ', '地', ' ', ' ', '给', ' ', ' ', '楼主', ' ', ' ', '泄气', ' ', ' ', '。'], ['家园网', ' ', ' ', '提供', ' ', ' ', '12M', ' ', ' ', '免费', ' ', ' ', '主页', ' ', ' ', '空间', ' ', ' ', '，', ' ', ' ', '欢迎', ' ', ' ', '申请', ' ', ' ', '家园网', ' ', ' ', '空间', ' ', ' ', '特点', ' ', ' ', '：', ' ', ' ', '独立', ' ', ' ', '二级域名', ' ', ' ', '，', ' ', ' ', 'Web', ' ', ' ', '上传', ' ', ' ', '，', ' ', ' ', '马上', ' ', ' ', '申请', ' ', ' ', '立即', ' ', ' ', '开通', ' ', ' ', '，', ' ', ' ', '永久', ' ', ' ', '免费', ' ', ' ', '终生', ' ', ' ', '稳定', ' ', ' ', '安全', ' ', ' ', '无广告', ' ', ' ', '客服', ' ', ' ', '在线', ' ', ' ', '答疑', ' ', ' ', '。', ' ', ' ', '网址', ' ', ' ', 'http', ' ', ' ', 'www', ' ', ' ', 'jaya', ' ', ' ', 'cn', ' ', ' ', '客服', ' ', ' ', '信箱', ' ', ' ', 'geduo', ' ', ' ', 'yeah', ' ', ' ', 'net', ' ', ' ', '网络实名', ' ', ' ', '：', ' ', ' ', '家园网', ' ', ' ', '家园网', ' ', ' ', '客服', ' ', ' ', '中心'], ['中信', ' ', ' ', '（', ' ', ' ', '国际', ' ', ' ', '）', ' ', ' ', '电子科技', ' ', ' ', '有限公司', ' ', ' ', '推出', ' ', ' ', '新', ' ', ' ', '产品', ' ', ' ', '：', ' ', ' ', '升职', ' ', ' ', '步步高', ' ', ' ', '、', ' ', ' ', '做生意', ' ', ' ', '发大财', ' ', ' ', '、', ' ', ' ', '连', ' ', ' ', '找', ' ', ' ', '情人', ' ', ' ', '都', ' ', ' ', '用', ' ', ' ', '的', ' ', ' ', '上', ' ', ' ', '，', ' ', ' ', '详情', ' ', ' ', '进入', ' ', ' ', '网', ' ', ' ', '址', ' ', ' ', 'http', ' ', ' ', 'www', ' ', ' ', 'usa5588', ' ', ' ', 'com', ' ', ' ', 'ccc', ' ', ' ', '电话', ' ', ' ', '：', ' ', ' ', '020', ' ', ' ', '33770208', ' ', ' ', '服务', ' ', ' ', '热线', ' ', ' ', '：', ' ', ' ', '013650852999']]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T11:16:38.339564Z",
     "start_time": "2024-04-08T11:16:38.125392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('基于词袋模型的多项朴素贝叶斯')\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb, train_features=bow_train_features,\n",
    "                                                   train_labels=train_labels, test_features=bow_test_features,\n",
    "                                                   test_labels=test_labels)\n",
    "\n",
    "# 输出结果：\n",
    "# 自己跑\n",
    "\n",
    "# 基于词袋模型特征的逻辑回归\n",
    "print('基于词袋模型特征的逻辑回归')\n",
    "lr_bow_predictions = train_predict_evaluate_model(classifier=lr, train_features=bow_train_features,\n",
    "                                                  train_labels=train_labels, test_features=bow_test_features,\n",
    "                                                  test_labels=test_labels)\n",
    "# 输出结果：\n",
    "# 自己跑\n",
    "\n",
    "# 基于词袋模型的支持向量机方法\n",
    "print('基于词袋模型的支持向量机方法')\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm, train_features=bow_train_features,\n",
    "                                                   train_labels=train_labels, test_features=bow_test_features,\n",
    "                                                   test_labels=test_labels)\n",
    "\n",
    "# 输出结果：\n",
    "# 自己跑\n",
    "\n",
    "# 基于tf-idf特征的多项朴素贝叶斯\n",
    "print('基于tf-idf特征的多项朴素贝叶斯')\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb, train_features=tfidf_train_features,\n",
    "                                                     train_labels=train_labels, test_features=tfidf_test_features,\n",
    "                                                     test_labels=test_labels)\n",
    "\n",
    "# 输出结果：\n",
    "# 自己跑\n",
    "\n",
    "# 基于tf-idf特征的逻辑回归\n",
    "print('基于tf-idf特征的逻辑回归')\n",
    "lr_tfidf_predictions = train_predict_evaluate_model(classifier=lr, train_features=tfidf_train_features,\n",
    "                                                    train_labels=train_labels, test_features=tfidf_test_features,\n",
    "                                                    test_labels=test_labels)\n",
    "\n",
    "# 输出结果：\n",
    "# 自己跑\n",
    "\n",
    "# 基于tf-idf特征的支持向量机方法\n",
    "print('基于tf-idf特征的支持向量机方法')\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm, train_features=tfidf_train_features,\n",
    "                                                     train_labels=train_labels, test_features=tfidf_test_features,\n",
    "                                                     test_labels=test_labels)\n"
   ],
   "id": "d2015712209d6e2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于词袋模型的多项朴素贝叶斯\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "F1 Score: 0.99\n",
      "基于词袋模型特征的逻辑回归\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "F1 Score: 0.99\n",
      "基于词袋模型的支持向量机方法\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "F1 Score: 0.99\n",
      "基于tf-idf特征的多项朴素贝叶斯\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "F1 Score: 0.99\n",
      "基于tf-idf特征的逻辑回归\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "F1 Score: 0.99\n",
      "基于tf-idf特征的支持向量机方法\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "F1 Score: 0.99\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T11:16:38.348739Z",
     "start_time": "2024-04-08T11:16:38.343089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 0 and predicted_label == 0:\n",
    "        print('邮件类型：', label_name_map[int(label)])\n",
    "        print('预测的邮件类型：', label_name_map[int(predicted_label)])\n",
    "        print('文本：-')\n",
    "        print(re.sub('\\n', '', document))\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 1 and predicted_label == 1:\n",
    "        print('邮件类型：', label_name_map[int(label)])\n",
    "        print('预测的邮件类型：', label_name_map[int(predicted_label)])\n",
    "        print('文本：-')\n",
    "        print(re.sub('\\n', '', document))\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break\n"
   ],
   "id": "40f4c8b36e6772ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "邮件类型： 垃圾邮件\n",
      "预测的邮件类型： 垃圾邮件\n",
      "文本：-\n",
      "中信（国际）电子科技有限公司推出新产品： 升职步步高、做生意发大财、连找情人都用的上，详情进入 网  址:  http://www.usa5588.com/ccc 电话：020-33770208   服务热线：013650852999\n",
      "邮件类型： 垃圾邮件\n",
      "预测的邮件类型： 垃圾邮件\n",
      "文本：-\n",
      "您好！ 我公司有多余的发票可以向外代开！（国税、地税、运输、广告、海关缴款书）。 如果贵公司（厂）有需要请来电洽谈、咨询！ 联系电话: 013510251389  陈先生 谢谢 顺祝商祺!\n",
      "邮件类型： 垃圾邮件\n",
      "预测的邮件类型： 垃圾邮件\n",
      "文本：-\n",
      "如果您在信箱中不能正常阅读此邮件，请点击这里\n",
      "邮件类型： 垃圾邮件\n",
      "预测的邮件类型： 垃圾邮件\n",
      "文本：-\n",
      "以下不能正确显示请点此 IFRAME: http://bbs.ewzw.com/viewthread.php?tid=3790\n",
      "邮件类型： 正常邮件\n",
      "预测的邮件类型： 正常邮件\n",
      "文本：-\n",
      "分专业吧，也分导师吧 标  题: Re: 问一个：有人觉得自己博士能混毕业吗 当然很好混毕业了 : 博士读到快中期了，始终感觉什么都不会，文章也没发几篇好的，论文的架构也没有， : 一切跟刚上的时候没有区别。但是事实上我也很辛苦的找资料，做实验，还进公司实习过， : 现在感觉好失败，内心已经放弃了，打算混毕业，不知道过来人有什么高招，请指点一二。 --\n",
      "邮件类型： 正常邮件\n",
      "预测的邮件类型： 正常邮件\n",
      "文本：-\n",
      "三年的水木生活,版大一直都照着我 我正是在伊的关照下逐步逐步的迈入水车行列 可惜我刚来贵版,伊就要走了 留个小整,以作纪念 版大真是个好人 恩 robertwc (我不想要nick) 共上站 1289 次，发表过 48999 篇文章 上次在  [Wed Sep  7 14:26:34 2005] 从 [211.151.90.88] 到本站一游。 离线时间[因在线上或非常断线不详] 信箱：[  ] 生命力： 身份: [版主]。 目前在站上，状态如下：\n",
      "邮件类型： 正常邮件\n",
      "预测的邮件类型： 正常邮件\n",
      "文本：-\n",
      "谢谢你:) 可能是这个浮躁的社会促使人也浮躁道德丧失，所以现在的婚姻失败的很多。相爱的人不一定在婚姻中能相扶相伴走很远。不仅你周围，每个有点社会阅历的人都能举出身边一大堆形形色色的例子。 可是人总是要尝试，万一自己是个幸福的特例呢。 我前一阵家里也是一塌糊涂，也有一个牙牙学语的小孩，所以特能理解你的心情，离婚不可怕，最难的是面对天真无邪的孩子\n",
      "邮件类型： 正常邮件\n",
      "预测的邮件类型： 正常邮件\n",
      "文本：-\n",
      "1，搜索文件，看是否不小心拖到某个地方了。 2，看回收站，看是否不小心删了。 3，动用文件恢复软件工具。 真是要疯了,存在电脑上的很多东西,包括什么爱情故事，原来写作的东西，还有很多心情日记,都不见了...完全没有搞清楚怎么回事,一整个文件夹都不知道跑哪里去了... 伤心到家了啊...发现有个快捷方式也还在就是东西不见了,,,哪个高人帮忙找出来啊??? 身家姓名啊,..有的东西失去了以后连回忆都没得回忆了...敬请指点....请大吃一顿,...\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
